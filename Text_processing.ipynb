{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код соберет в одном месте следующие функции для обработки текста на русском языке в формате класса:\n",
    "чистка от пунктуации,\n",
    "лемматизация,\n",
    "стемматизация,\n",
    "удаление стоп-слов,\n",
    "поиск самых частотных н-грам,\n",
    "сентенизация,\n",
    "токенизация,\n",
    "выделение и нормализация именованных сущностей,\n",
    "возвращение формата UD,\n",
    "анализ тональности, \n",
    "и вывод общей статистики по тексту.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erokh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#все импорты\n",
    "import spacy\n",
    "nlp=spacy.load(\"ru_core_news_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, idx, form, lemma, pos, feats, head, deprel):\n",
    "        self.idx = idx\n",
    "        self.form = form\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        self.feats = feats\n",
    "        self.head = head\n",
    "        self.deprel = deprel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_ru: \n",
    "    def __init__(self, data, name):\n",
    "        self.data=data\n",
    "        self.name=name\n",
    "\n",
    "    def write(self, spec, done): #записать в соответствующий файл то, что сделали\n",
    "        with open(self.name+'_'+spec+'.txt', 'w', encoding='UTF-8') as f:\n",
    "            f.write(str(done))\n",
    "    \n",
    "    def clean(self): #очистить от пунктуации и переносов строк\n",
    "        data=self.data\n",
    "        data=re.sub(r'[^\\w\\s]','',data)\n",
    "        data=re.sub(r'\\n', '', data)\n",
    "        return data\n",
    "    \n",
    "    def lemmatize(self): #лемматизировать с помощью функции токенизировать внизу, лемматизация от спейси не очень хорошая, но не хочется использовать слишком много библиотек\n",
    "        lemmas=[]\n",
    "        tokens=self.tokenize()\n",
    "        for t in tokens:\n",
    "            if t.pos != 'PUNCT':\n",
    "                lemmas.append(t.lemma)\n",
    "        return lemmas\n",
    "        \n",
    "    def stemmatize(self): #стемматизировать с помощью nltk\n",
    "        stemmer=SnowballStemmer('russian')  \n",
    "        stems=[]\n",
    "        data=self.clean()\n",
    "        tokens=nltk.word_tokenize(data)\n",
    "        for token in tokens:\n",
    "            stems.append(stemmer.stem(token))\n",
    "        return stems\n",
    "\n",
    "    def delete_stopwords(self): #удалить стопслова с помощью nltk и вернуть текст для дальнейшего использования\n",
    "        data=self.data\n",
    "        stop_words=set(stopwords.words('russian'))\n",
    "        tokens=word_tokenize(data)\n",
    "        filtered=[w for w in tokens if not w.lower() in stop_words]\n",
    "        return ' '.join(filtered)\n",
    "    \n",
    "    def find_common_ngrams(self, n=3, common=10):\n",
    "        data=self.clean()\n",
    "        tokens=nltk.word_tokenize(data)\n",
    "        text_nltk=Text(tokens)\n",
    "        c=Counter(list(ngrams(text_nltk, n)))\n",
    "        return c.most_common(common)\n",
    "\n",
    "    def sentenize(self): #сентенизировать с помощью spacy\n",
    "        data=self.data\n",
    "        doc=nlp(data)\n",
    "        sents=[sent.text for sent in doc.sents]\n",
    "        return sents  \n",
    "\n",
    "    def tokenize(self): #токенизировать с помощью spacy и создать объекты класса Token для удобного использования\n",
    "        tokens=[]\n",
    "        data=self.data\n",
    "        doc=nlp(data)\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_=='ROOT':\n",
    "                    head=0\n",
    "                else:\n",
    "                    head=token.head.i-sent.start+1\n",
    "                id=token.i - sent.start+1\n",
    "                t=Token(id, token.text, token.lemma_, token.pos_, str(token.morph), head, token.dep_,)\n",
    "                tokens.append(t)\n",
    "        return tokens\n",
    "\n",
    "    def extract_ent(self): #выделить именнованные сущности\n",
    "        ents=[]\n",
    "        data=self.data\n",
    "        doc=nlp(data)\n",
    "        for ent in doc.ents:\n",
    "            ents.append([ent.text, ent.label_])\n",
    "        return ents\n",
    "    \n",
    "    def to_UD(self): #напечатать в формате UD\n",
    "        tokens=self.tokenize()\n",
    "        for t in tokens:\n",
    "            print(t.idx, \n",
    "                t.form, \n",
    "                t.lemma, \n",
    "                t.pos, \n",
    "                '_', \n",
    "                t.feats, \n",
    "                t.head,\n",
    "                t.deprel,\n",
    "                '_',\n",
    "                '_', sep='\\t')\n",
    "    \n",
    "    def sent_analyze(self): #модель не может анализировать слишком большой текст, поэтому делаем это попредложенно.\n",
    "        sentiment_pipeline = pipeline('sentiment-analysis', model='blanchefort/rubert-base-cased-sentiment')\n",
    "        results=[]\n",
    "        sents=self.sentenize()\n",
    "        for sent in sents:\n",
    "            result = sentiment_pipeline(sent)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def stats(self):\n",
    "        #токены и предложения\n",
    "        aver = round(len(self.tokenize())/len(self.sentenize()))\n",
    "        unique = round(len(set(self.tokenize()))/len(self.tokenize())*100)\n",
    "        print(f'Кол-во токенов {len(self.tokenize())}\\nКол-во предложений {len(self.sentenize())}\\nСредняя длина предложения {aver}\\n')\n",
    "\n",
    "        #статистика по pos\n",
    "        poses = set()\n",
    "        for t in self.tokenize():\n",
    "            poses.add(t.pos)\n",
    "        poses = list(poses)\n",
    "\n",
    "        for i in range(len(poses)):\n",
    "            quant_i = 0\n",
    "            for t in self.tokenize():\n",
    "                if t.pos == poses[i]:\n",
    "                    quant_i+=1\n",
    "            print(f'{poses[i]}: {quant_i} токен(а/ов)')\n",
    "        \n",
    "        #леммы и стемы\n",
    "        c_lemms=Counter(self.lemmatize())\n",
    "        c_stems=Counter(self.stemmatize())\n",
    "        print(f'\\nКол-во лемм: {len(self.lemmatize())}\\nКол-во уникальных лемм: {len(c_lemms)}\\n10 самымх частотных лем: {c_lemms.most_common(10)}')\n",
    "        print(f'Кол-во стем: {len(self.stemmatize())}\\nКол-во уникальных стем: {len(c_stems)}\\n10 самымх частотных стем: {c_stems.most_common(10)}\\n')\n",
    "\n",
    "        #н-граммы\n",
    "        print(f'Самые частотные биграммы: {self.find_common_ngrams(2, 5)}\\nСамые частотные триграммы: {self.find_common_ngrams(3, 5)}\\n')\n",
    "\n",
    "        #стоп слова\n",
    "        data=self.data\n",
    "        stop_words=set(stopwords.words('russian'))\n",
    "        tokens=word_tokenize(data)\n",
    "        sw_text=[w for w in tokens if w.lower() in stop_words]\n",
    "        c_sw=Counter(sw_text)\n",
    "        print(f'В тексте можно удалить {len(sw_text)} стопслов(а).\\nСамые частотные из них {c_sw.most_common(5)}\\n')\n",
    "\n",
    "        #именнованные сущности\n",
    "        tags=[]\n",
    "        names=[]\n",
    "        ents=self.extract_ent()\n",
    "        for i in range(len(ents)):\n",
    "            tags.append(ents[i][1])\n",
    "            names.append(ents[i][0])\n",
    "        c_tags=Counter(tags)\n",
    "        c_names=Counter(names)\n",
    "        print(f'Кол-во именнованных сущностей: {len(ents)}\\nРаспределения по тегам: PER - {c_tags[\"PER\"]}, LOC - {c_tags[\"LOC\"]}\\nСамые частотные именнованные сущности {c_names.most_common(5)}\\n')\n",
    "\n",
    "        #анализ тональности:\n",
    "        labels=[]\n",
    "        result=self.sent_analyze()\n",
    "        for i in range(len(result)):\n",
    "            labels.append(result[i][0]['label'])\n",
    "        c_labels=Counter(labels)\n",
    "        print(f'Кол-во позитивных предложений - {c_labels[\"POSITIVE\"]}, нейтральных - {c_labels[\"NEUTRAL\"]} и негативных - {c_labels[\"NEGATIVE\"]}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metel.clean()\\nmetel.stemmatize()\\nmetel.tokenize()\\nmetel.sentenize()\\nmetel.extract_ent()\\nmetel.delete_stopwords()\\nmetel.find_common_ngrams()\\nmetel.extract_ent()\\nmetel.to_UD()\\nmetel.sent_analyze()'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('project_ru.txt', 'r', encoding='UTF-8') as f:\n",
    "    data=f.read()\n",
    "metel=Text_ru(data, 'project_ru')\n",
    "\n",
    "'''metel.clean()\n",
    "metel.stemmatize()\n",
    "metel.tokenize()\n",
    "metel.sentenize()\n",
    "metel.extract_ent()\n",
    "metel.delete_stopwords()\n",
    "metel.find_common_ngrams()\n",
    "metel.extract_ent()\n",
    "metel.to_UD()\n",
    "metel.sent_analyze()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во токенов 4402\n",
      "Кол-во предложений 267\n",
      "Средняя длина предложения 16\n",
      "\n",
      "ADP: 328 токен(а/ов)\n",
      "ADV: 225 токен(а/ов)\n",
      "ADJ: 264 токен(а/ов)\n",
      "PUNCT: 900 токен(а/ов)\n",
      "PRON: 359 токен(а/ов)\n",
      "PROPN: 178 токен(а/ов)\n",
      "DET: 112 токен(а/ов)\n",
      "AUX: 86 токен(а/ов)\n",
      "NUM: 38 токен(а/ов)\n",
      "SCONJ: 72 токен(а/ов)\n",
      "X: 9 токен(а/ов)\n",
      "PART: 122 токен(а/ов)\n",
      "NOUN: 787 токен(а/ов)\n",
      "VERB: 668 токен(а/ов)\n",
      "CCONJ: 190 токен(а/ов)\n",
      "SPACE: 64 токен(а/ов)\n",
      "\n",
      "Кол-во лемм: 3502\n",
      "Кол-во уникальных лемм: 1369\n",
      "10 самымх частотных лем: [('и', 147), ('в', 110), ('не', 68), ('что', 59), ('\\n', 58), ('она', 56), ('он', 52), ('быть', 51), ('с', 44), ('гаврилович', 32)]\n",
      "Кол-во стем: 3355\n",
      "Кол-во уникальных стем: 1272\n",
      "10 самымх частотных стем: [('и', 146), ('в', 101), ('он', 93), ('не', 80), ('был', 76), ('что', 54), ('е', 49), ('с', 44), ('на', 31), ('владимир', 30)]\n",
      "\n",
      "Самые частотные биграммы: [(('Марья', 'Гавриловна'), 17), (('и', 'с'), 6), (('Гаврила', 'Гаврилович'), 5), (('и', 'что'), 4), (('Марьи', 'Гавриловны'), 4)]\n",
      "Самые частотные триграммы: [(('было', 'не', 'видать'), 3), (('в', 'свою', 'комнату'), 2), (('Марья', 'Гавриловна', 'не'), 2), (('ее', 'сердце', 'Она'), 2), (('Метель', 'не', 'утихала'), 2)]\n",
      "\n",
      "В тексте можно удалить 1240 стопслов(а).\n",
      "Самые частотные из них [('и', 145), ('в', 96), ('не', 66), ('что', 49), ('с', 43)]\n",
      "\n",
      "Кол-во именнованных сущностей: 118\n",
      "Распределения по тегам: PER - 105, LOC - 13\n",
      "Самые частотные именнованные сущности [('Владимир', 22), ('Марья Гавриловна', 20), ('Маша', 9), ('Бурмин', 8), ('Владимира', 6)]\n",
      "\n",
      "Кол-во позитивных предложений - 30, нейтральных - 139 и негативных - 98.\n"
     ]
    }
   ],
   "source": [
    "metel.stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
